{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892ee471",
   "metadata": {},
   "source": [
    "## 4. ML Implementation & Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef78523",
   "metadata": {},
   "source": [
    "### Part-One Build ML solutions for performance, availability, scalability, resiliency, and fault tolerance\n",
    "\n",
    "**Important Topics**\n",
    "* Amazon Deep Learning containers\n",
    "* AWS Deep Learning AMI (Amazon Machine Image)\n",
    "* AWS Auto Scaling\n",
    "* AWS GPU (P2 and P3) and CPU instances\n",
    "* Amazon CloudWatch\n",
    "* AWS CloudTrail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a46002",
   "metadata": {},
   "source": [
    "**High availability and fault tolerance**<br>\n",
    "In a highly available solution, the system will continue to function even when any component of the architecture stops working. A key aspect of high availability is fault tolerance, which, when built into an architecture, ensures that applications will continue to function without degradation in performance, despite the complete failure of any component of the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50574d",
   "metadata": {},
   "source": [
    "**Key AWS Services that can be used to maintain High availability:**<br>\n",
    "1. Amazon Glue and Amazon EMR \n",
    "You should decouple your ETL process from the ML pipeline. The compute power needed for ML isn’t the same as what you’d need for an ETL process—they have very different requirements. \n",
    "You can make use of this decoupled architecture by simply using an ETL service like AWS Glue or Amazon EMR, which use Apache Spark for your ETL jobs and Amazon SageMaker to train, test, and deploy your models.\n",
    "\n",
    "2. Amazon Sagemaker Endpoints\n",
    "To ensure a highly available ML serving endpoint, deploy Amazon SageMaker endpoints backed by multiple instances across Availability Zones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eac99d",
   "metadata": {},
   "source": [
    "### a. Amazon Cloud Watch\n",
    "**Amazon CloudWatch** helps you monitor your system while storing all the logs and operational metrics separately from the actual implementation and code for training and testing your ML models.\n",
    "\n",
    "**Amazon CloudWatch Events** delivers a near-real-time stream of system events that describe changes in AWS resources\n",
    "\n",
    "**Amazon CloudWatch alarms** allows you to watch CloudWatch metrics and to receive notifications when the metrics fall outside of the levels (high or low thresholds) that you configure\n",
    "\n",
    "\n",
    "Amazon SageMaker provides out-of-the-box integration with Amazon CloudWatch, which collects near-real-time utilization metrics for the training job instance, such as CPU, memory, and GPU utilization of the training job container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8202079b",
   "metadata": {},
   "source": [
    "### b. AWS CloudTrail\n",
    "**AWS CloudTrail** captures API calls and related events made by or on behalf of your AWS account and delivers the log files to an Amazon S3 bucket that you specify. You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61c12d",
   "metadata": {},
   "source": [
    "### c. AWS Auto Scaling\n",
    "With AWS Auto Scaling, you configure and manage scaling for your resources through a scaling plan. The scaling plan uses dynamic scaling and predictive scaling to automatically scale your application’s resources. The scaling plan lets you choose scaling strategies to define how to optimize your resource utilization. You can optimize for availability, for cost, or a balance of both. \n",
    "\n",
    "To determine the scaling policy for automatic scaling in Amazon SageMaker, test for how much load Recieve Packet Steering (RPS) the endpoint can sustain. Then configure automatic scaling and observe how the model behaves when it scales out. Expected behavior is lower latency and fewer or no errors with automatic scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67b540",
   "metadata": {},
   "source": [
    "## Part-Two Recommend and implement the appropriate ML services and features for a given problem\n",
    "\n",
    "**Important Topics**\n",
    "* Amazon SageMaker Spark containers\n",
    "* Amazon SageMaker build your own containers\n",
    "* Amazon AI services\n",
    "    * Amazon Translate\n",
    "    * Amazon Lex\n",
    "    * Amazon Polly\n",
    "    * Amazon Transcribe\n",
    "    * Amazon Rekognition\n",
    "    * Amazon Comprehend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39e684",
   "metadata": {},
   "source": [
    "**Data Ingestion and Transformation Services**\n",
    "* AWS Glue \n",
    "* Amazon EMR\n",
    "* Amazon Kinesis\n",
    "\n",
    "**Model Building, Training, Tuning, and Evaluation Services**\n",
    "* Amazon SageMaker\n",
    "\n",
    "**Amazon AI Services**<br>\n",
    "* Rekognition - Vision, Image Analysis, Face Recognition, Image classification\n",
    "* Polly - Speech, Text to speech, speech enabled products\n",
    "* Lex - Used to build conversational application for voice and text\n",
    "* Translate - Delivers fast, high-quality, affordable, and customizable language translation.\n",
    "* Transcribe - Automatically convert speech to text\n",
    "* Comprehend - Derive and understand valuable insights from text within documents\n",
    "* Forecast - A time-series forecasting service based on machine learning (ML) and built for business metrics analysis.\n",
    "* Personalize - Quickly build and deploy curated recommendations and intelligent user segmentation at scale using machine learning (ML). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b66af62",
   "metadata": {},
   "source": [
    "## Part-Three Apply Basic AWS security practices to ML solutions\n",
    "\n",
    "**Important Topics**\n",
    "* Security on Amazon SageMaker\n",
    "* Infrastructure security on Amazon SageMaker\n",
    "* What is a:\n",
    "    * VPC\n",
    "    * Security group\n",
    "    * NAT gateway\n",
    "    * Internet gateway\n",
    "* AWS Key Management Service (AWS KMS)\n",
    "* AWS Identity and Access Management (IAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c4797",
   "metadata": {},
   "source": [
    "There are two ways to use AWS KMS with Amazon S3:\n",
    "1. Client Side\n",
    "2. Server Side\n",
    "    * **SSE-S3** requires that Amazon S3 manage the data and master encryption keys.\n",
    "    * **SSE-C** requires that you manage the encryption key. \n",
    "    * **SSE-KMS** requires that AWS manage the data key, but you manage the customer master key in AWS KMS. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9e17f",
   "metadata": {},
   "source": [
    "## Part-Four Deploy and operationalize ML solutions\n",
    "\n",
    "**Important Topics**\n",
    "* A/B testing with Amazon SageMaker\n",
    "* Amazon SageMaker endpoints\n",
    "    * Production variants\n",
    "    * Endpoint configuration\n",
    "* Using Lambda with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd8af8",
   "metadata": {},
   "source": [
    "**Deploying a model using Amazon SageMaker hosting services is a three-step process**<br>\n",
    "1. Create a model in Amazon SageMaker - You need:\n",
    "    * The Amazon S3 path where the model artifacts are stored \n",
    "    * The Docker registry path for the image that contains the inference code \n",
    "    * A name that you can use for subsequent deployment steps\n",
    "<br>\n",
    "2. Create an endpoint configuration for an HTTPS endpoint - You need:\n",
    "    * The name of one or more models in production variants\n",
    "    * The ML compute instances that you want Amazon SageMaker to launch to host each production variant. When hosting models in production, you can configure the endpoint to elastically scale the deployed ML compute instances. For each production variant, you specify the number of ML compute instances that you want to deploy. When you specify two or more instances, Amazon SageMaker launches them in multiple Availability Zones. This ensures continuous availability. Amazon SageMaker manages deploying the instances.\n",
    "<br>\n",
    "3. Create an HTTPS endpoint - You need:\n",
    "    * To provide the endpoint configuration to Amazon SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
