{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e26afd",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec7a4ad",
   "metadata": {},
   "source": [
    "## Part-One Frame business problems as ML problems\n",
    "\n",
    "**Important Topics**\n",
    "* Supervised learning\n",
    "    * Regression and classification\n",
    "* Unsupervised learning\n",
    "    * Clustering\n",
    "    * Anomaly detection\n",
    "* Deep learning\n",
    "    * Perceptron\n",
    "    * Components of an artificial neuron\n",
    "\n",
    "First, remember that ML is about identifying hidden patterns in data. It has the potential to leverage large amounts of data to train an ML model on that data’s patterns and structures to then make predictions. And the power of ML is that your model, in theory, gets progressively better at making these predictions as it’s trained.\n",
    "\n",
    "ML is not appropriate when you can determine a target value by using simple rules or computations that can be programmed without needing any data-driven learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714543a",
   "metadata": {},
   "source": [
    "### a. Supervised Learning\n",
    "Supervised learning is a popular type of ML because it’s widely applicable and has several successful applications. Supervised algorithms learn patterns by seeing the relationships between variables and known outcomes. Take a simple image recognition example. You provide your model with training data that includes images of different animals and the corresponding labels, which essentially give the model the correct answer for each label. This one is a cat. This one is a dog. After training on this data, in theory, your model should be able to predict the type of animal it sees in a totally new picture that it encounters in production. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a539579",
   "metadata": {},
   "source": [
    "### b. Unsupervised Learning\n",
    "By contrast, when you don’t have training data labeled and you don’t already understand how inputs may map to outputs, you might want to look to unsupervised learning as a solution. \n",
    "A common type of unsupervised learning is called clustering. This kind of algorithm groups data points into different clusters based on similar features in order to better understand the attributes of a specific group or cluster. For instance, let’s say you sell office supplies to different companies. In analyzing your customer purchasing habits, unsupervised learning might be able to identify, let’s say, two different groups of customers without the need for specific labels. Maybe the model identifies that the one cluster that is centered around purchasing products like paper and pencils happens to be smaller companies. Whereas the cluster that is centered around products like conference tables and chairs is made up of larger companies. In this situation, clustering might help you realize that you need to come up with a different marketing strategy for different-sized companies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cace963",
   "metadata": {},
   "source": [
    "## Part-Two Select the appropriate model(s) for an ML problem\n",
    "\n",
    "**Important Topics**\n",
    "* Linear learner\n",
    "* XGBoost\n",
    "* K-means\n",
    "* Decision trees\n",
    "* Random forest\n",
    "* Image classification\n",
    "* Object detection\n",
    "* Semantic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4741f12",
   "metadata": {},
   "source": [
    "**I am not going into the details in each one of them as I am expecting you already know what each of them do and where to use them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d44115",
   "metadata": {},
   "source": [
    "## Part-Three Train ML models\n",
    "\n",
    "**Important Topics**\n",
    "\n",
    "* Amazon SageMaker workflow for training jobs\n",
    "* Running a training job using containers\n",
    "* Build your own containers\n",
    "* Amazon EC2 P3 instances\n",
    "* Components of an ML training job for deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc5bb3",
   "metadata": {},
   "source": [
    "Split data to ensure a proper division between training and evaluation\n",
    "\n",
    "**Cross-validation**<br>\n",
    "Use cross-validation methods to compare the performance of multiple models. The goal behind cross-validation is to help you choose the model that will eventually perform the best in production. \n",
    "\n",
    "**K-fold cross-validation**<br>\n",
    "K-fold cross-validation is a common validation method. In k-fold cross-validation, you split the input data into k subsets of data (also known as folds). You train your models on all but one (k-1) of the subsets, and then evaluate them on the subset that was not used for training. This process is repeated k times, with a different subset reserved for evaluation (and excluded from training) each time. \n",
    "\n",
    "**Stratified K-fold cross-validation**<br>\n",
    "Stratified k-fold cross-validation is the same as just k-fold cross-validation, But Stratified k-fold cross-validation, it does stratified sampling instead of random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3495f0",
   "metadata": {},
   "source": [
    "### a. Amazon EC2 P3\n",
    "Amazon EC2 P3 instances deliver high performance compute in the cloud with up to 8 NVIDIA® V100 Tensor Core GPUs and up to 100 Gbps of networking throughput for machine learning and HPC applications. These instances deliver up to one petaflop of mixed-precision performance per instance to significantly accelerate machine learning and high performance computing applications. Amazon EC2 P3 instances have been proven to reduce machine learning training times from days to minutes, as well as increase the number of simulations completed for high performance computing by 3-4x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f146c",
   "metadata": {},
   "source": [
    "### Part-Four Perform hyperparameter optimization\n",
    "\n",
    "**Important Topics**\n",
    "* Amazon SageMaker hyperparameter tuning jobs\n",
    "* Common hyperparameters to tune:\n",
    "    * Momentum\n",
    "    * Optimizers\n",
    "    * Activation functions\n",
    "    * Dropout\n",
    "    * Learning rate\n",
    "* Regularization:\n",
    "    * Dropout\n",
    "    * L1/L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61adf9",
   "metadata": {},
   "source": [
    "**What are hyperparameters?**<br>\n",
    "Hyperparameters are the settings that can be tuned before running a training job to control the behavior of an ML algorithm. They can have a big impact on model training as it relates to training time, model convergence, and model accuracy. Unlike model parameters that are derived from the training job, the values of hyperparameters do not change during the training. \n",
    "\n",
    "Categories of hyperparameters:<br>\n",
    "**1. Model hyperparameters**<br>\n",
    "Model hyperparameters define the model itself—Attributes of a neural network architecture like filter size, pooling, stride, padding.\n",
    "\n",
    "**2. Optimizer hyperparameters**<br>\n",
    "Optimizer hyperparameters, are related to how the model learn the patterns based on data and are used for a neural network model. These types of hyperparameters include optimizers like gradient descent and stochastic gradient descent, or even optimizers using momentum like Adam or initializing the parameter weights using methods like Xavier initialization or He initialization\n",
    "\n",
    "**3. Data hyperparameters**<br>\n",
    "Data hyperparameters are related to the attributes of the data, often used when you don’t have enough data or enough variation in data—Data augmentation techniques like cropping, resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e74f15",
   "metadata": {},
   "source": [
    "**Grid search**\n",
    "Grid search is one of those methods. With grid search, you set up a grid made up of hyperparameters and their different values. For each possible combination, a model is trained and a score is produced on the validation data. With this approach, every single combination of the given possible hyperparameter values is tried. This approach, while thorough, can be very inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce869a2",
   "metadata": {},
   "source": [
    "**Random search**\n",
    "Random search is similar to grid search, but instead of training and scoring on each possible hyperparameter combination, random combinations are selected. You can set the number of search iterations based on time and resource constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c998df3",
   "metadata": {},
   "source": [
    "### a. Amazon SageMaker hyperparameter tuning jobs\n",
    "Amazon SageMaker lets you perform automated hyperparameter tuning. Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb9c34",
   "metadata": {},
   "source": [
    "### b. Regularization \n",
    "Regularization  is one of the most important concepts of machine learning. It is a technique to prevent the model from overfitting by adding extra information to it.\n",
    "\n",
    "**Ridge Regression or L2 regularization**<br>\n",
    "Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization. In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. We can calculate it by multiplying with the lambda to the squared weight of each individual feature.\n",
    "It helps to solve the problems if we have more parameters than samples.\n",
    "\n",
    "**Lasso Regression or Least Absolute and Selection Operator or L1 regularization**<br>\n",
    "It is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights. Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0. \n",
    "The Lasso regression can help us to reduce the overfitting in the model as well as the feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518f7c0",
   "metadata": {},
   "source": [
    "### Part-Five Evaluate ML models\n",
    "\n",
    "**Important Topics**\n",
    "* Metrics for regression: sum of squared errors, RMSE\n",
    "* Sensitivity\n",
    "* Specificity\n",
    "* Neural network functions like Softmax for the last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e0607",
   "metadata": {},
   "source": [
    "**For classification problems, a confusion matrix is the building block for your model evaluation. **\n",
    "\n",
    "Metrics for classification problems:<br>\n",
    "**1. Accuracy**<br>\n",
    "Accuracy is the ratio of correct predictions to total number of predictions\n",
    "\n",
    "**2. Precision**<br>\n",
    "Precision is the proportion of positive predictions that are actually correct\n",
    "\n",
    "**3. Recall**<br>\n",
    "Recall is the proportion of correct sets that are identified as positive\n",
    "\n",
    "**4. F1 score**<br>\n",
    "2* Precision* Recall/ (Precision+Recall)\n",
    "\n",
    "**5. Area Under Curve**<br>\n",
    "AUC-ROC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
